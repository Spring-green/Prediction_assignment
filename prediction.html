

PMLProject

BY

22/05/2020


Project

It is now possible to collect a large amount of data about personal activity relatively inexpensively Using devices such as Jawbone Up, Nike FuelBand, and Fitbit. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

The goal of this project is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. The participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The process of building the model, using cross validation, the expected out of sample error, the reasons for the selecting choices are explained in this report.

More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Dataset). The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har.


Cleaning data

There are some variables in both datasets which include missing values. There are also some variables which looks have no effect on the outcome (user name, stamptime, window, etc.).Also, Some variables have no or very little variability in them and will likely not be good predictors. To identify those variables we used nearzerovar function. Then, We removed all of them.


Creating train and test data for prediction
## [1] 13737    53
## [1] 5885   53


Building Model

To predict the outcome, we used the following methods: 1. Classification tree 2. Random forest, and 3. Gradient boosting.

We used cross-validation technique for evaluating the models.


1. Classification tree

Building a model with classification tree algorithm, we’ve found an accuracy of about 50% which is low. As a result, the model can’t predict Classe with a proper accuracy using all other predictors. Generalization error or Out of sample error is 0.5 which is so high.
## n= 13737 
## 
## node), split, n, loss, yval, (yprob)
##       * denotes terminal node
## 
##  1) root 13737 9831 A (0.28 0.19 0.17 0.16 0.18)  
##    2) roll_belt< 129.5 12478 8620 A (0.31 0.21 0.19 0.18 0.11)  
##      4) pitch_forearm< -34 1099    7 A (0.99 0.0064 0 0 0) *
##      5) pitch_forearm>=-34 11379 8613 A (0.24 0.23 0.21 0.2 0.12)  
##       10) magnet_dumbbell_y< 439.5 9626 6914 A (0.28 0.18 0.24 0.19 0.1)  
##         20) roll_forearm< 123.5 6017 3594 A (0.4 0.18 0.19 0.17 0.055) *
##         21) roll_forearm>=123.5 3609 2422 C (0.08 0.18 0.33 0.23 0.18) *
##       11) magnet_dumbbell_y>=439.5 1753  853 B (0.031 0.51 0.047 0.23 0.18) *
##    3) roll_belt>=129.5 1259   48 E (0.038 0 0 0 0.96) *
## CART 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Bootstrapped (25 reps) 
## Summary of sample sizes: 13737, 13737, 13737, 13737, 13737, 13737, ... 
## Resampling results across tuning parameters:
## 
##   cp          Accuracy   Kappa     
##   0.03631370  0.5070151  0.36100388
##   0.05913268  0.4137320  0.20329082
##   0.11829926  0.3179383  0.05025736
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.0363137.


## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1504   27  117    0   26
##          B  470  386  283    0    0
##          C  460   26  540    0    0
##          D  419  172  373    0    0
##          E  158  145  291    0  488
## 
## Overall Statistics
##                                          
##                Accuracy : 0.4958         
##                  95% CI : (0.483, 0.5087)
##     No Information Rate : 0.5116         
##     P-Value [Acc > NIR] : 0.9926         
##                                          
##                   Kappa : 0.3418         
##                                          
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.4995  0.51058  0.33666       NA  0.94942
## Specificity            0.9408  0.85319  0.88648   0.8362  0.88941
## Pos Pred Value         0.8984  0.33889  0.52632       NA  0.45102
## Neg Pred Value         0.6421  0.92204  0.78102       NA  0.99459
## Prevalence             0.5116  0.12846  0.27256   0.0000  0.08734
## Detection Rate         0.2556  0.06559  0.09176   0.0000  0.08292
## Detection Prevalence   0.2845  0.19354  0.17434   0.1638  0.18386
## Balanced Accuracy      0.7202  0.68188  0.61157       NA  0.91941


2. Random forest

Building a model with random forest algorithm, we’ve found an accuracy of about 99.3% which is high. As a result, the model can predict Classe with a proper accuracy using all other predictors. Generalization error or Out of sample error is 0.01 which is quiet low.
## 
## Call:
##  randomForest(x = x, y = y, mtry = param$mtry) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 27
## 
##         OOB estimate of  error rate: 0.69%
## Confusion matrix:
##      A    B    C    D    E class.error
## A 3897    6    2    0    1 0.002304147
## B   17 2635    5    1    0 0.008653123
## C    0    9 2375   12    0 0.008764608
## D    0    1   28 2223    0 0.012877442
## E    0    2    3    8 2512 0.005148515
## Random Forest 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10990, 10990, 10991, 10988, 10989 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##    2    0.9906820  0.9882116
##   27    0.9910458  0.9886730
##   52    0.9833308  0.9789100
## 
## Accuracy was used to select the optimal model using the largest value.
## The final value used for the model was mtry = 27.
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1672    2    0    0    0
##          B    8 1130    1    0    0
##          C    0    7 1017    2    0
##          D    0    0   10  953    1
##          E    0    1    5    7 1069
## 
## Overall Statistics
##                                         
##                Accuracy : 0.9925        
##                  95% CI : (0.99, 0.9946)
##     No Information Rate : 0.2855        
##     P-Value [Acc > NIR] : < 2.2e-16     
##                                         
##                   Kappa : 0.9905        
##                                         
##  Mcnemar's Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9952   0.9912   0.9845   0.9906   0.9991
## Specificity            0.9995   0.9981   0.9981   0.9978   0.9973
## Pos Pred Value         0.9988   0.9921   0.9912   0.9886   0.9880
## Neg Pred Value         0.9981   0.9979   0.9967   0.9982   0.9998
## Prevalence             0.2855   0.1937   0.1755   0.1635   0.1818
## Detection Rate         0.2841   0.1920   0.1728   0.1619   0.1816
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9974   0.9947   0.9913   0.9942   0.9982


3. Gradient Boosting

Building a model with gradient boosting algorithm, we’ve found an accuracy of about 96.4% which is high. As a result, the model can predict Classe with a proper accuracy using all other predictors. Generalization error or Out of sample error is 0.04 which is quiet low.
## A gradient boosted model with multinomial loss function.
## 150 iterations were performed.
## There were 52 predictors of which 52 had non-zero influence.
## Stochastic Gradient Boosting 
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 10989, 10991, 10990, 10989, 10989 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  Accuracy   Kappa    
##   1                   50      0.7482684  0.6808682
##   1                  100      0.8204087  0.7726279
##   1                  150      0.8538955  0.8151534
##   2                   50      0.8573893  0.8193207
##   2                  100      0.9073288  0.8826958
##   2                  150      0.9311333  0.9128539
##   3                   50      0.8957533  0.8680125
##   3                  100      0.9407425  0.9250057
##   3                  150      0.9598156  0.9491605
## 
## Tuning parameter 'shrinkage' was held constant at a value of 0.1
## 
## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1654   11    4    3    2
##          B   33 1079   24    3    0
##          C    0   24  990   11    1
##          D    0    3   34  925    2
##          E    5    8    6   27 1036
## 
## Overall Statistics
##                                           
##                Accuracy : 0.9658          
##                  95% CI : (0.9609, 0.9703)
##     No Information Rate : 0.2875          
##     P-Value [Acc > NIR] : < 2.2e-16       
##                                           
##                   Kappa : 0.9568          
##                                           
##  Mcnemar's Test P-Value : 5.855e-10       
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9775   0.9591   0.9357   0.9546   0.9952
## Specificity            0.9952   0.9874   0.9925   0.9921   0.9905
## Pos Pred Value         0.9881   0.9473   0.9649   0.9595   0.9575
## Neg Pred Value         0.9910   0.9903   0.9860   0.9911   0.9990
## Prevalence             0.2875   0.1912   0.1798   0.1647   0.1769
## Detection Rate         0.2811   0.1833   0.1682   0.1572   0.1760
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9864   0.9733   0.9641   0.9733   0.9929


Best model

We selected Random Forest model for prediction as it has the highest accuracy among all models.

We applied the model to test data then.
##  [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
      
